{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english-text\\abcrn-aboriginalcorona.txt\n",
      "english-text\\abcrn-indigenousmemory.txt\n",
      "english-text\\abcrn-indigenousperception.txt\n",
      "english-text\\abcrn-maritime.txt\n",
      "english-text\\abcrn-psychologist.txt\n",
      "english-text\\aus-shortstories.txt\n",
      "english-text\\bible-joshua.txt\n",
      "english-text\\chifley-warisover.txt\n",
      "english-text\\curtin-warwithjapan.txt\n",
      "english-text\\english-100-words - Copy.txt\n",
      "english-text\\english-100-words.txt\n",
      "english-text\\english-movie-top-100-words - Copy.txt\n",
      "english-text\\english-movie-top-100-words.txt\n",
      "english-text\\english-movie-top-200-words - Copy.txt\n",
      "english-text\\english-movie-top-200-words.txt\n",
      "english-text\\english-movie-top-50-words - Copy.txt\n",
      "english-text\\english-movie-top-50-words.txt\n",
      "english-text\\english-movie-words.txt\n",
      "english-text\\english-top-200-words - Copy.txt\n",
      "english-text\\english-top-200-words.txt\n",
      "english-text\\english-top-50-words - Copy.txt\n",
      "english-text\\english-top-50-words.txt\n",
      "english-text\\english-words.txt\n",
      "english-text\\hinkler-message.txt\n",
      "english-text\\keating-redfern.txt\n",
      "english-text\\rudd-apology.txt\n",
      "Unique word count: 6478\n",
      "the and of to that in it is you we was for my are with have on they as be this all people from So about at but me not there up she or what an her had our by will out them their when Aboriginal like were can your he us if has time Australia been do just way because more which know very one ve things would then think into those Indigenous his how who unto said say these LORD children also over first go now really back before come country get shall much world life see place make where Joshua no actually even years re around book came day could home than Australian every well down two don right might got water want look men man need through different something other good work didn old take him went any did great after Israel land new made ye only some took never Jordan too most course story lot health many same landscape find may still little memory tell mean here part done Mum Abramovic always thou give once high long let culture found feel son own put next thought understand off family moment novel away again three heart ark communities language often bit myself launch priests example am set name should words side pass help everything later call house past few big risk point between ask person told seven God city talk someone remember hard love sense information another sea use ll sure end upon felt write until MoMA last social why better important kids history turn mother non issues night war since each able hand must ever father kind death self school thee quite across services best without while mental given leave ships live move keep fact left does today United mind brought wasn against sorry whole knowledge week writer early become nothing learn saw trauma wrong real free air York thy under number enough case far Lynne started woman read longer walk anything knew isn show Dr though future pain behind Moses Jericho nation else group along maybe 000 research heard 10 Pacific head idea sound English both terms believe Government seem sat psychologists began face stand light hear less months care yet hope earth held learnt beginning sort four human wrote imagine space news stop full child ourselves beautiful training form safety car second cause 12 taken soon port practitioners cannot yes feet swim covenant Japan close teach University particular okay brain strong watching river small reality happen guess international page power Sydney six spent lost morning Torres Strait 19 Pat pretty trying such change ago sent returned traditional location deal evidence States doesn add almost identity flying relationship view certain yeah friends test vessel met among assessment common written open recognise boy hath trumpets accursed COVID Commonwealth situation seen elders absolutely obviously stuff couldn law wouldn town author round oh thousands sun fire follow mainstream wonder 20 truth white Westerman suicide young problem line hadn gave fear Japanese surgery commanded midst thanks run South north decided already bad Kelly order window sing therefore exactly proud sometimes gone themselves gull probably throughout duck nature build instead yourself picture lucky sitting conversation wait based cruise black racism meet die act blood 13 17 joy late dream stood fully fighting public neither Giuliano atrium 11 fine women journalist stones Islander continuing whatever within haven herself complex generation connection guy system art itself age piece explain babies area Melbourne during reason safe \n",
      "\n",
      "Top 574 most common words:\n",
      "the 2373\n",
      "and 1440\n",
      "of 1263\n",
      "to 1131\n",
      "that 814\n",
      "in 775\n",
      "it 473\n",
      "is 450\n",
      "you 391\n",
      "we 352\n",
      "was 343\n",
      "for 340\n",
      "my 312\n",
      "are 288\n",
      "with 269\n",
      "have 255\n",
      "on 251\n",
      "they 243\n",
      "as 241\n",
      "be 238\n",
      "this 214\n",
      "all 200\n",
      "people 198\n",
      "from 192\n",
      "So 186\n",
      "about 179\n",
      "at 167\n",
      "but 163\n",
      "me 152\n",
      "not 151\n",
      "there 141\n",
      "up 141\n",
      "she 140\n",
      "or 139\n",
      "what 135\n",
      "an 132\n",
      "her 131\n",
      "had 129\n",
      "our 128\n",
      "by 124\n",
      "will 123\n",
      "out 122\n",
      "them 121\n",
      "their 116\n",
      "when 116\n",
      "Aboriginal 110\n",
      "like 110\n",
      "were 110\n",
      "can 107\n",
      "your 107\n",
      "he 107\n",
      "us 106\n",
      "if 106\n",
      "has 101\n",
      "time 101\n",
      "Australia 100\n",
      "been 99\n",
      "do 98\n",
      "just 94\n",
      "way 93\n",
      "because 92\n",
      "more 92\n",
      "which 91\n",
      "know 88\n",
      "very 87\n",
      "one 87\n",
      "ve 86\n",
      "things 86\n",
      "would 85\n",
      "then 85\n",
      "think 83\n",
      "into 81\n",
      "those 81\n",
      "Indigenous 80\n",
      "his 80\n",
      "how 79\n",
      "who 79\n",
      "unto 79\n",
      "said 76\n",
      "say 73\n",
      "these 73\n",
      "LORD 73\n",
      "children 71\n",
      "also 70\n",
      "over 70\n",
      "first 69\n",
      "go 69\n",
      "now 68\n",
      "really 68\n",
      "back 68\n",
      "before 67\n",
      "come 66\n",
      "country 65\n",
      "get 65\n",
      "shall 65\n",
      "much 64\n",
      "world 64\n",
      "life 64\n",
      "see 63\n",
      "place 62\n",
      "make 62\n",
      "where 61\n",
      "Joshua 61\n",
      "no 60\n",
      "actually 59\n",
      "even 58\n",
      "years 58\n",
      "re 57\n",
      "around 57\n",
      "book 57\n",
      "came 57\n",
      "day 57\n",
      "could 56\n",
      "home 56\n",
      "than 55\n",
      "Australian 53\n",
      "every 52\n",
      "well 51\n",
      "down 51\n",
      "two 51\n",
      "don 50\n",
      "right 50\n",
      "might 50\n",
      "got 49\n",
      "water 49\n",
      "want 48\n",
      "look 48\n",
      "men 47\n",
      "man 46\n",
      "need 46\n",
      "through 46\n",
      "different 46\n",
      "something 46\n",
      "other 45\n",
      "good 45\n",
      "work 45\n",
      "didn 45\n",
      "old 45\n",
      "take 45\n",
      "him 45\n",
      "went 44\n",
      "any 43\n",
      "did 43\n",
      "great 42\n",
      "after 42\n",
      "Israel 42\n",
      "land 41\n",
      "new 41\n",
      "made 41\n",
      "ye 41\n",
      "only 40\n",
      "some 40\n",
      "took 39\n",
      "never 39\n",
      "Jordan 39\n",
      "too 38\n",
      "most 38\n",
      "course 38\n",
      "story 38\n",
      "lot 37\n",
      "health 37\n",
      "many 37\n",
      "same 37\n",
      "landscape 36\n",
      "find 36\n",
      "may 36\n",
      "still 35\n",
      "little 35\n",
      "memory 34\n",
      "tell 34\n",
      "mean 34\n",
      "here 33\n",
      "part 33\n",
      "done 33\n",
      "Mum 33\n",
      "Abramovic 33\n",
      "always 32\n",
      "thou 32\n",
      "give 31\n",
      "once 31\n",
      "high 31\n",
      "long 31\n",
      "let 31\n",
      "culture 31\n",
      "found 31\n",
      "feel 31\n",
      "son 31\n",
      "own 30\n",
      "put 30\n",
      "next 30\n",
      "thought 30\n",
      "understand 30\n",
      "off 30\n",
      "family 29\n",
      "moment 29\n",
      "novel 29\n",
      "away 28\n",
      "again 28\n",
      "three 28\n",
      "heart 28\n",
      "ark 28\n",
      "communities 27\n",
      "language 27\n",
      "often 27\n",
      "bit 27\n",
      "myself 27\n",
      "launch 27\n",
      "priests 27\n",
      "example 26\n",
      "am 26\n",
      "set 26\n",
      "name 26\n",
      "should 26\n",
      "words 26\n",
      "side 26\n",
      "pass 26\n",
      "help 25\n",
      "everything 25\n",
      "later 25\n",
      "call 25\n",
      "house 25\n",
      "past 25\n",
      "few 25\n",
      "big 25\n",
      "risk 24\n",
      "point 24\n",
      "between 24\n",
      "ask 24\n",
      "person 24\n",
      "told 24\n",
      "seven 24\n",
      "God 24\n",
      "city 24\n",
      "talk 23\n",
      "someone 23\n",
      "remember 23\n",
      "hard 23\n",
      "love 23\n",
      "sense 22\n",
      "information 22\n",
      "another 22\n",
      "sea 22\n",
      "use 22\n",
      "ll 22\n",
      "sure 22\n",
      "end 22\n",
      "upon 22\n",
      "felt 22\n",
      "write 22\n",
      "until 22\n",
      "MoMA 22\n",
      "last 21\n",
      "social 21\n",
      "why 21\n",
      "better 21\n",
      "important 21\n",
      "kids 21\n",
      "history 21\n",
      "turn 21\n",
      "mother 21\n",
      "non 21\n",
      "issues 21\n",
      "night 21\n",
      "war 21\n",
      "since 20\n",
      "each 20\n",
      "able 20\n",
      "hand 20\n",
      "must 20\n",
      "ever 20\n",
      "father 20\n",
      "kind 20\n",
      "death 20\n",
      "self 20\n",
      "school 20\n",
      "thee 20\n",
      "quite 19\n",
      "across 19\n",
      "services 19\n",
      "best 19\n",
      "without 19\n",
      "while 19\n",
      "mental 19\n",
      "given 19\n",
      "leave 19\n",
      "ships 19\n",
      "live 18\n",
      "move 18\n",
      "keep 18\n",
      "fact 18\n",
      "left 18\n",
      "does 18\n",
      "today 18\n",
      "United 18\n",
      "mind 18\n",
      "brought 18\n",
      "wasn 18\n",
      "against 18\n",
      "sorry 17\n",
      "whole 17\n",
      "knowledge 17\n",
      "week 17\n",
      "writer 17\n",
      "early 17\n",
      "become 17\n",
      "nothing 17\n",
      "learn 17\n",
      "saw 17\n",
      "trauma 17\n",
      "wrong 17\n",
      "real 17\n",
      "free 17\n",
      "air 17\n",
      "York 17\n",
      "thy 17\n",
      "under 16\n",
      "number 16\n",
      "enough 16\n",
      "case 16\n",
      "far 16\n",
      "Lynne 16\n",
      "started 16\n",
      "woman 16\n",
      "read 16\n",
      "longer 16\n",
      "walk 16\n",
      "anything 16\n",
      "knew 16\n",
      "isn 16\n",
      "show 16\n",
      "Dr 16\n",
      "though 16\n",
      "future 16\n",
      "pain 16\n",
      "behind 16\n",
      "Moses 16\n",
      "Jericho 16\n",
      "nation 15\n",
      "else 15\n",
      "group 15\n",
      "along 15\n",
      "maybe 15\n",
      "000 15\n",
      "research 15\n",
      "heard 15\n",
      "10 15\n",
      "Pacific 15\n",
      "head 15\n",
      "idea 15\n",
      "sound 15\n",
      "English 15\n",
      "both 15\n",
      "terms 15\n",
      "believe 15\n",
      "Government 15\n",
      "seem 15\n",
      "sat 15\n",
      "psychologists 15\n",
      "began 15\n",
      "face 15\n",
      "stand 15\n",
      "light 15\n",
      "hear 14\n",
      "less 14\n",
      "months 14\n",
      "care 14\n",
      "yet 14\n",
      "hope 14\n",
      "earth 14\n",
      "held 14\n",
      "learnt 14\n",
      "beginning 14\n",
      "sort 14\n",
      "four 14\n",
      "human 14\n",
      "wrote 14\n",
      "imagine 14\n",
      "space 14\n",
      "news 14\n",
      "stop 14\n",
      "full 14\n",
      "child 14\n",
      "ourselves 14\n",
      "beautiful 14\n",
      "training 14\n",
      "form 14\n",
      "safety 14\n",
      "car 14\n",
      "second 14\n",
      "cause 14\n",
      "12 14\n",
      "taken 14\n",
      "soon 14\n",
      "port 14\n",
      "practitioners 14\n",
      "cannot 14\n",
      "yes 14\n",
      "feet 14\n",
      "swim 14\n",
      "covenant 14\n",
      "Japan 14\n",
      "close 13\n",
      "teach 13\n",
      "University 13\n",
      "particular 13\n",
      "okay 13\n",
      "brain 13\n",
      "strong 13\n",
      "watching 13\n",
      "river 13\n",
      "small 13\n",
      "reality 13\n",
      "happen 13\n",
      "guess 13\n",
      "international 13\n",
      "page 13\n",
      "power 13\n",
      "Sydney 13\n",
      "six 13\n",
      "spent 13\n",
      "lost 13\n",
      "morning 13\n",
      "Torres 12\n",
      "Strait 12\n",
      "19 12\n",
      "Pat 12\n",
      "pretty 12\n",
      "trying 12\n",
      "such 12\n",
      "change 12\n",
      "ago 12\n",
      "sent 12\n",
      "returned 12\n",
      "traditional 12\n",
      "location 12\n",
      "deal 12\n",
      "evidence 12\n",
      "States 12\n",
      "doesn 12\n",
      "add 12\n",
      "almost 12\n",
      "identity 12\n",
      "flying 12\n",
      "relationship 12\n",
      "view 12\n",
      "certain 12\n",
      "yeah 12\n",
      "friends 12\n",
      "test 12\n",
      "vessel 12\n",
      "met 12\n",
      "among 12\n",
      "assessment 12\n",
      "common 12\n",
      "written 12\n",
      "open 12\n",
      "recognise 12\n",
      "boy 12\n",
      "hath 12\n",
      "trumpets 12\n",
      "accursed 12\n",
      "COVID 11\n",
      "Commonwealth 11\n",
      "situation 11\n",
      "seen 11\n",
      "elders 11\n",
      "absolutely 11\n",
      "obviously 11\n",
      "stuff 11\n",
      "couldn 11\n",
      "law 11\n",
      "wouldn 11\n",
      "town 11\n",
      "author 11\n",
      "round 11\n",
      "oh 11\n",
      "thousands 11\n",
      "sun 11\n",
      "fire 11\n",
      "follow 11\n",
      "mainstream 11\n",
      "wonder 11\n",
      "20 11\n",
      "truth 11\n",
      "white 11\n",
      "Westerman 11\n",
      "suicide 11\n",
      "young 11\n",
      "problem 11\n",
      "line 11\n",
      "hadn 11\n",
      "gave 11\n",
      "fear 11\n",
      "Japanese 11\n",
      "surgery 11\n",
      "commanded 11\n",
      "midst 11\n",
      "thanks 11\n",
      "run 10\n",
      "South 10\n",
      "north 10\n",
      "decided 10\n",
      "already 10\n",
      "bad 10\n",
      "Kelly 10\n",
      "order 10\n",
      "window 10\n",
      "sing 10\n",
      "therefore 10\n",
      "exactly 10\n",
      "proud 10\n",
      "sometimes 10\n",
      "gone 10\n",
      "themselves 10\n",
      "gull 10\n",
      "probably 10\n",
      "throughout 10\n",
      "duck 10\n",
      "nature 10\n",
      "build 10\n",
      "instead 10\n",
      "yourself 10\n",
      "picture 10\n",
      "lucky 10\n",
      "sitting 10\n",
      "conversation 10\n",
      "wait 10\n",
      "based 10\n",
      "cruise 10\n",
      "black 10\n",
      "racism 10\n",
      "meet 10\n",
      "die 10\n",
      "act 10\n",
      "blood 10\n",
      "13 10\n",
      "17 10\n",
      "joy 10\n",
      "late 10\n",
      "dream 10\n",
      "stood 10\n",
      "fully 10\n",
      "fighting 10\n",
      "public 10\n",
      "neither 10\n",
      "Giuliano 10\n",
      "atrium 10\n",
      "11 10\n",
      "fine 10\n",
      "women 10\n",
      "journalist 10\n",
      "stones 10\n",
      "Islander 9\n",
      "continuing 9\n",
      "whatever 9\n",
      "within 9\n",
      "haven 9\n",
      "herself 9\n",
      "complex 9\n",
      "generation 9\n",
      "connection 9\n",
      "guy 9\n",
      "system 9\n",
      "art 9\n",
      "itself 9\n",
      "age 9\n",
      "piece 9\n",
      "explain 9\n",
      "babies 9\n",
      "area 9\n",
      "Melbourne 9\n",
      "during 9\n",
      "reason 9\n",
      "safe 9\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from collections import Counter\n",
    "import glob\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "import nltk\n",
    "from os import path\n",
    "\n",
    "aus_txt = []\n",
    "stop_words = ['s', 're', 'mm']\n",
    "\n",
    "for txt in sorted(glob.glob(path.join('english-text','*.txt'))):\n",
    "    txt_file = open(txt, \"r\")\n",
    "    print (txt)\n",
    "    transcript = txt_file.read()\n",
    "    for words in tokenizer.tokenize(transcript):\n",
    "        aus_txt.append(words)\n",
    "#stopwords = set(stopwords.words('english'))\n",
    "\n",
    "print \n",
    "tf = nltk.FreqDist(aus_txt)\n",
    "print('Unique word count: '+str(len(tf)))\n",
    "word_list = []\n",
    "stem_list = []\n",
    "\n",
    "for value, count in tf.most_common(700):\n",
    "    if stemmer.stem(value.lower()) in stem_list or len(value)==1:\n",
    "        continue #print('Skipped: '+value) \n",
    "    else:\n",
    "        word_list.append((value,count))\n",
    "        stem_list.append(stemmer.stem(value.lower()))\n",
    "        print(value, end =\" \")\n",
    "print('\\n')\n",
    "print('Top ' + str(len(word_list)) + ' most common words:')\n",
    "\n",
    "for value, count in word_list:\n",
    "    print(value, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'the', 'audio': 'english_words/common/common_word_01.wav'}, {'word': 'of', 'audio': 'english_words/common/common_word_02.wav'}, {'word': 'to', 'audio': 'english_words/common/common_word_03.wav'}, {'word': 'and', 'audio': 'english_words/common/common_word_04.wav'}, {'word': 'a', 'audio': 'english_words/common/common_word_05.wav'}, {'word': 'in', 'audio': 'english_words/common/common_word_06.wav'}, {'word': 'is', 'audio': 'english_words/common/common_word_07.wav'}, {'word': 'it', 'audio': 'english_words/common/common_word_08.wav'}, {'word': 'you', 'audio': 'english_words/common/common_word_09.wav'}, {'word': 'that', 'audio': 'english_words/common/common_word_10.wav'}, {'word': 'he', 'audio': 'english_words/common/common_word_11.wav'}, {'word': 'was', 'audio': 'english_words/common/common_word_12.wav'}, {'word': 'for', 'audio': 'english_words/common/common_word_13.wav'}, {'word': 'on', 'audio': 'english_words/common/common_word_14.wav'}, {'word': 'are', 'audio': 'english_words/common/common_word_15.wav'}, {'word': 'with', 'audio': 'english_words/common/common_word_16.wav'}, {'word': 'as', 'audio': 'english_words/common/common_word_17.wav'}, {'word': 'i', 'audio': 'english_words/common/common_word_18.wav'}, {'word': 'his', 'audio': 'english_words/common/common_word_19.wav'}, {'word': 'they', 'audio': 'english_words/common/common_word_20.wav'}, {'word': 'be', 'audio': 'english_words/common/common_word_21.wav'}, {'word': 'at', 'audio': 'english_words/common/common_word_22.wav'}, {'word': 'one', 'audio': 'english_words/common/common_word_23.wav'}, {'word': 'have', 'audio': 'english_words/common/common_word_24.wav'}, {'word': 'this', 'audio': 'english_words/common/common_word_25.wav'}, {'word': 'from', 'audio': 'english_words/common/common_word_26.wav'}, {'word': 'or', 'audio': 'english_words/common/common_word_27.wav'}, {'word': 'had', 'audio': 'english_words/common/common_word_28.wav'}, {'word': 'by', 'audio': 'english_words/common/common_word_29.wav'}, {'word': 'word', 'audio': 'english_words/common/common_word_30.wav'}, {'word': 'but', 'audio': 'english_words/common/common_word_31.wav'}, {'word': 'what', 'audio': 'english_words/common/common_word_32.wav'}, {'word': 'some', 'audio': 'english_words/common/common_word_33.wav'}, {'word': 'we', 'audio': 'english_words/common/common_word_34.wav'}, {'word': 'can', 'audio': 'english_words/common/common_word_35.wav'}, {'word': 'out', 'audio': 'english_words/common/common_word_36.wav'}, {'word': 'other', 'audio': 'english_words/common/common_word_37.wav'}, {'word': 'were', 'audio': 'english_words/common/common_word_38.wav'}, {'word': 'all', 'audio': 'english_words/common/common_word_39.wav'}, {'word': 'there', 'audio': 'english_words/common/common_word_40.wav'}, {'word': 'when', 'audio': 'english_words/common/common_word_41.wav'}, {'word': 'up', 'audio': 'english_words/common/common_word_42.wav'}, {'word': 'use', 'audio': 'english_words/common/common_word_43.wav'}, {'word': 'your', 'audio': 'english_words/common/common_word_44.wav'}, {'word': 'how', 'audio': 'english_words/common/common_word_45.wav'}, {'word': 'said', 'audio': 'english_words/common/common_word_46.wav'}, {'word': 'an', 'audio': 'english_words/common/common_word_47.wav'}, {'word': 'each', 'audio': 'english_words/common/common_word_48.wav'}, {'word': 'she', 'audio': 'english_words/common/common_word_49.wav'}, {'word': 'which', 'audio': 'english_words/common/common_word_50.wav'}, {'word': 'do', 'audio': 'english_words/common/common_word_51.wav'}, {'word': 'their', 'audio': 'english_words/common/common_word_52.wav'}, {'word': 'time', 'audio': 'english_words/common/common_word_53.wav'}, {'word': 'if', 'audio': 'english_words/common/common_word_54.wav'}, {'word': 'will', 'audio': 'english_words/common/common_word_55.wav'}, {'word': 'way', 'audio': 'english_words/common/common_word_56.wav'}, {'word': 'about', 'audio': 'english_words/common/common_word_57.wav'}, {'word': 'many', 'audio': 'english_words/common/common_word_58.wav'}, {'word': 'then', 'audio': 'english_words/common/common_word_59.wav'}, {'word': 'them', 'audio': 'english_words/common/common_word_60.wav'}, {'word': 'write', 'audio': 'english_words/common/common_word_61.wav'}, {'word': 'would', 'audio': 'english_words/common/common_word_62.wav'}, {'word': 'like', 'audio': 'english_words/common/common_word_63.wav'}, {'word': 'so', 'audio': 'english_words/common/common_word_64.wav'}, {'word': 'these', 'audio': 'english_words/common/common_word_65.wav'}, {'word': 'her', 'audio': 'english_words/common/common_word_66.wav'}, {'word': 'long', 'audio': 'english_words/common/common_word_67.wav'}, {'word': 'make', 'audio': 'english_words/common/common_word_68.wav'}, {'word': 'thing', 'audio': 'english_words/common/common_word_69.wav'}, {'word': 'see', 'audio': 'english_words/common/common_word_70.wav'}, {'word': 'him', 'audio': 'english_words/common/common_word_71.wav'}, {'word': 'two', 'audio': 'english_words/common/common_word_72.wav'}, {'word': 'has', 'audio': 'english_words/common/common_word_73.wav'}, {'word': 'look', 'audio': 'english_words/common/common_word_74.wav'}, {'word': 'more', 'audio': 'english_words/common/common_word_75.wav'}, {'word': 'day', 'audio': 'english_words/common/common_word_76.wav'}, {'word': 'could', 'audio': 'english_words/common/common_word_77.wav'}, {'word': 'go', 'audio': 'english_words/common/common_word_78.wav'}, {'word': 'come', 'audio': 'english_words/common/common_word_79.wav'}, {'word': 'did', 'audio': 'english_words/common/common_word_80.wav'}, {'word': 'number', 'audio': 'english_words/common/common_word_81.wav'}, {'word': 'sound', 'audio': 'english_words/common/common_word_82.wav'}, {'word': 'no', 'audio': 'english_words/common/common_word_83.wav'}, {'word': 'most', 'audio': 'english_words/common/common_word_84.wav'}, {'word': 'people', 'audio': 'english_words/common/common_word_85.wav'}, {'word': 'my', 'audio': 'english_words/common/common_word_86.wav'}, {'word': 'over', 'audio': 'english_words/common/common_word_87.wav'}, {'word': 'know', 'audio': 'english_words/common/common_word_88.wav'}, {'word': 'water', 'audio': 'english_words/common/common_word_89.wav'}, {'word': 'than', 'audio': 'english_words/common/common_word_90.wav'}, {'word': 'call', 'audio': 'english_words/common/common_word_91.wav'}, {'word': 'first', 'audio': 'english_words/common/common_word_92.wav'}, {'word': 'who', 'audio': 'english_words/common/common_word_93.wav'}, {'word': 'may', 'audio': 'english_words/common/common_word_94.wav'}, {'word': 'down', 'audio': 'english_words/common/common_word_95.wav'}, {'word': 'side', 'audio': 'english_words/common/common_word_96.wav'}, {'word': 'been', 'audio': 'english_words/common/common_word_97.wav'}, {'word': 'now', 'audio': 'english_words/common/common_word_98.wav'}, {'word': 'find', 'audio': 'english_words/common/common_word_99.wav'}, {'word': 'any', 'audio': 'english_words/common/common_word_100.wav'}]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "i = 1\n",
    "json_base = []\n",
    "folder_dir = 'english_words/common/'\n",
    "for word in wordlist[:100]:\n",
    "    json_ref = {}\n",
    "    json_ref[\"word\"] = word\n",
    "    json_ref[\"audio\"] = folder_dir+'common_word_'+str(\"%02d\"%i)+'.wav'\n",
    "    json_base.append(json_ref)\n",
    "    i+=1\n",
    "print(json_base)\n",
    "with io.open(\"common_words_100.json\", mode='w', encoding='utf-8') as dump_json_file:\n",
    "    json.dump(json_base, dump_json_file, ensure_ascii=False)\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'aboriginal', 'audio': 'english_words/redfern/redfern_word_01.wav'}, {'word': 'australia', 'audio': 'english_words/redfern/redfern_word_02.wav'}, {'word': 'australians', 'audio': 'english_words/redfern/redfern_word_03.wav'}, {'word': 'imagine', 'audio': 'english_words/redfern/redfern_word_04.wav'}, {'word': 'us', 'audio': 'english_words/redfern/redfern_word_05.wav'}, {'word': 'indigenous', 'audio': 'english_words/redfern/redfern_word_06.wav'}, {'word': 'people', 'audio': 'english_words/redfern/redfern_word_07.wav'}, {'word': 'cannot', 'audio': 'english_words/redfern/redfern_word_08.wav'}, {'word': 'think', 'audio': 'english_words/redfern/redfern_word_09.wav'}, {'word': 'things', 'audio': 'english_words/redfern/redfern_word_10.wav'}, {'word': 'know', 'audio': 'english_words/redfern/redfern_word_11.wav'}, {'word': 'justice', 'audio': 'english_words/redfern/redfern_word_12.wav'}, {'word': 'world', 'audio': 'english_words/redfern/redfern_word_13.wav'}, {'word': 'say', 'audio': 'english_words/redfern/redfern_word_14.wav'}, {'word': 'recognise', 'audio': 'english_words/redfern/redfern_word_15.wav'}, {'word': 'much', 'audio': 'english_words/redfern/redfern_word_16.wav'}, {'word': 'non', 'audio': 'english_words/redfern/redfern_word_17.wav'}, {'word': 'year', 'audio': 'english_words/redfern/redfern_word_18.wav'}, {'word': 'torres', 'audio': 'english_words/redfern/redfern_word_19.wav'}, {'word': 'strait', 'audio': 'english_words/redfern/redfern_word_20.wav'}, {'word': 'social', 'audio': 'english_words/redfern/redfern_word_21.wav'}, {'word': 'injustice', 'audio': 'english_words/redfern/redfern_word_22.wav'}, {'word': 'see', 'audio': 'english_words/redfern/redfern_word_23.wav'}, {'word': 'well', 'audio': 'english_words/redfern/redfern_word_24.wav'}, {'word': 'history', 'audio': 'english_words/redfern/redfern_word_25.wav'}, {'word': 'identity', 'audio': 'english_words/redfern/redfern_word_26.wav'}, {'word': 'many', 'audio': 'english_words/redfern/redfern_word_27.wav'}, {'word': 'australian', 'audio': 'english_words/redfern/redfern_word_28.wav'}, {'word': 'reality', 'audio': 'english_words/redfern/redfern_word_29.wav'}, {'word': 'done', 'audio': 'english_words/redfern/redfern_word_30.wav'}]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"keating-redfern-top30.txt\", \"r\")\n",
    "doc = f.read()\n",
    "wordlist = []\n",
    "\n",
    "#stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokenizer.tokenize(doc):\n",
    "    wordlist.append(token.lower())\n",
    "\n",
    "import io\n",
    "import json\n",
    "i = 1\n",
    "json_base = []\n",
    "folder_dir = 'english_words/redfern/'\n",
    "for word in wordlist[:100]:\n",
    "    json_ref = {}\n",
    "    json_ref[\"word\"] = word\n",
    "    json_ref[\"audio\"] = folder_dir+'redfern_word_'+str(\"%02d\"%i)+'.wav'\n",
    "    json_base.append(json_ref)\n",
    "    i+=1\n",
    "print(json_base)\n",
    "with io.open(\"redfern_words_30.json\", mode='w', encoding='utf-8') as dump_json_file:\n",
    "    json.dump(json_base, dump_json_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50\n",
      "one word use said time way many write would like long make thing see two look day could go come number sound people know water call first may side find new work part take get place made live back little round man year came show every good give name form \n",
      "\n",
      "Top 100\n",
      "sentence great think say help low line differ turn cause much mean move right boy old tell set three want air well also play small end put home read hand port large spell add even land must big high follow act ask men change went light kind need house picture \n",
      "\n",
      "Top 200\n",
      "try us animal point mother world near build self earth father head stand page country found answer school grow study still learn plant cover food sun four state keep eye never last let thought city tree cross farm hard start might story saw far sea draw left late run press close night real life north open seem together next white children begin got walk example ease paper group always music mark often letter mile river car feet care second book carry took science eat room friend began idea fish mountain stop base hear horse cut sure watch color face wood \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Top 50')\n",
    "for word in wordlist[:50]:\n",
    "    print(word, end =\" \")\n",
    "print('\\n')\n",
    "print('Top 100')\n",
    "for word in wordlist[50:100]:\n",
    "    print(word, end =\" \")\n",
    "print('\\n')\n",
    "print('Top 200')\n",
    "for word in wordlist[100:200]:\n",
    "    print(word, end =\" \")    \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50\n",
      "know well oh right get going like yeah want think go got one see come good really would look time okay back mean tell hey could yes something say ok take way us little make need gonna never love sure sorry let thing maybe man uh anything said much life \n",
      "\n",
      "Top 100\n",
      "know well oh right get going like yeah want think go got one see come good really would look time okay back mean tell hey could yes something say ok take way us little make need gonna never love sure sorry let thing maybe man uh anything said much life even please thank give thought help two talk people god still wait find nothing things let call told great better ever night away first believe feel everything work fine home last day keep put around stop guy long always listen wanted mr guys huh big lot happened thanks trying kind \n",
      "\n",
      "Top 200\n",
      "know well oh right get going like yeah want think go got one see come good really would look time okay back mean tell hey could yes something say ok take way us little make need gonna never love sure sorry let thing maybe man uh anything said much life even please thank give thought help two talk people god still wait find nothing things let call told great better ever night away first believe feel everything work fine home last day keep put around stop guy long always listen wanted mr guys huh big lot happened thanks trying kind wrong talking made new guess hi care bad mom remember getting together dad leave mother place understand actually hear baby nice father else stay done course might mind every enough try hell came someone family whole another house jack idea ask best must coming old looking woman hello years room money left knew tonight real son hope name went um hmm happy pretty saw girl sir show friend already saying may next three job problem minute found world thinking heard honey matter exactly ah probably happen hurt boy dead gotta alone since excuse start kill hard today car ready without \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Top 50')\n",
    "for word in wordlist[:50]:\n",
    "    print(word, end =\" \")\n",
    "print('\\n')\n",
    "print('Top 100')\n",
    "for word in wordlist[:100]:\n",
    "    print(word, end =\" \")\n",
    "print('\\n')\n",
    "print('Top 200')\n",
    "for word in wordlist[:200]:\n",
    "    print(word, end =\" \")    \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
